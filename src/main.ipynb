{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":29550,"sourceType":"datasetVersion","datasetId":23079}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from pathlib import Path\nfrom datasets import load_dataset, Image\n\nDATA_DIR = Path(\"/kaggle/input/asl-alphabet\")\nTRAIN_DIR = DATA_DIR.joinpath(\"asl_alphabet_train\", \"asl_alphabet_train\")\n\nTRAIN_DATASET = load_dataset(\"imagefolder\", data_dir=TRAIN_DIR, split=\"train\")\nTRAIN_DATASET = TRAIN_DATASET.cast_column(\"image\", Image(mode=\"RGB\"))\nTRAIN_DATASET = TRAIN_DATASET.with_format(\"torch\")\nTRAIN_DATASET.save_to_disk(\"data/train\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-12T07:10:46.003790Z","iopub.execute_input":"2024-11-12T07:10:46.004232Z","iopub.status.idle":"2024-11-12T07:26:27.903071Z","shell.execute_reply.started":"2024-11-12T07:10:46.004190Z","shell.execute_reply":"2024-11-12T07:26:27.901988Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nDownloading data: 100%|██████████| 87000/87000 [00:03<00:00, 28420.37files/s] \nComputing checksums: 100%|██████████| 87000/87000 [00:23<00:00, 3694.84it/s] \nGenerating train split: 87000 examples [00:08, 10238.76 examples/s]\nSaving the dataset (3/3 shards): 100%|██████████| 87000/87000 [09:56<00:00, 145.88 examples/s]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import torch\nimport torchmetrics\nimport lightning as L\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom lightning.pytorch import callbacks\nfrom torchvision.transforms import v2 as transforms\n\n\nclass AslTranslator(L.LightningModule):\n    def __init__(\n        self,\n        input_dim: tuple[int, int, int],\n        output_dim: int,\n        lr=5e-5,\n        optimizer_cls: type[torch.optim.Optimizer] = torch.optim.SGD,\n        lr_scheduler_cls: type[\n            torch.optim.lr_scheduler.LRScheduler\n        ] = torch.optim.lr_scheduler.CosineAnnealingLR,\n        lr_scheduler_kwargs: dict[str, any] = {},\n    ):\n        super().__init__()\n        self.save_hyperparameters()\n\n        channels, _, _ = input_dim\n        decoy_tensor = torch.zeros(input_dim)\n\n        conv1 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=channels,\n                out_channels=channels * 2,\n                kernel_size=3,\n            ),\n            nn.AvgPool2d(kernel_size=3, stride=2),\n            nn.GELU(),\n            nn.Dropout(),\n        )\n\n        conv2 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=channels * 2,\n                out_channels=channels * 4,\n                kernel_size=3,\n            ),\n            nn.AvgPool2d(kernel_size=3, stride=2),\n            nn.GELU(),\n            nn.Dropout(),\n        )\n\n        conv3 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=channels * 4,\n                out_channels=channels * 8,\n                kernel_size=3,\n            ),\n            nn.AvgPool2d(kernel_size=3, stride=2),\n            nn.GELU(),\n            nn.Dropout(),\n        )\n\n        self.conv = nn.Sequential(conv1, conv2, conv3)\n        decoy_tensor: torch.Tensor = self.conv(decoy_tensor)\n        decoy_tensor = torch.flatten(decoy_tensor, start_dim=1)\n        hidden_size = decoy_tensor.size(1) // 2\n\n        linear1 = nn.Sequential(\n            nn.Linear(in_features=decoy_tensor.size(1), out_features=hidden_size),\n            nn.GELU(),\n            nn.Dropout(),\n        )\n\n        linear2 = nn.Sequential(\n            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n            nn.GELU(),\n            nn.Dropout(),\n        )\n\n        self.linear = nn.Sequential(linear1, linear2)\n        decoy_tensor: torch.Tensor = self.linear(decoy_tensor)\n\n        self.classifier = nn.Linear(\n            in_features=decoy_tensor.size(1), out_features=output_dim\n        )\n\n    def forward(self, inputs):\n        x = self.conv(inputs)\n        x = self.linear(x)\n        logits = self.classifier(x)\n        return logits\n\n    def training_step(self, examples, _):\n        features: torch.Tensor = examples[\"images\"]\n        targets: torch.Tensor = examples[\"labels\"]\n\n        logits: torch.Tensor = self(features)\n\n        loss = F.cross_entropy(logits, targets)\n\n        preds = logits.argmax(1)\n        f1 = torchmetrics.functional.f1_score(preds, targets, task=\"multiclass\")\n        acc = torchmetrics.functional.accuracy(preds, targets, task=\"multiclass\")\n        recall = torchmetrics.functional.recall(preds, targets, task=\"multiclass\")\n\n        self.log_dict(\n            {\n                \"train_loss\": loss.item(),\n                \"train_f1\": f1.item(),\n                \"train_acc\": acc.item(),\n                \"train_recall\": recall.item(),\n            },\n            prog_bar=True,\n            batch_size=targets.size(0),\n        )\n\n        return loss\n\n    def validation_step(self, examples, _):\n        features: torch.Tensor = examples[\"images\"]\n        targets: torch.Tensor = examples[\"labels\"]\n        logits: torch.Tensor = self(features)\n\n        loss = F.cross_entropy(logits, targets)\n\n        preds = logits.argmax(1)\n        f1 = torchmetrics.functional.f1_score(preds, targets, task=\"multiclass\")\n        acc = torchmetrics.functional.accuracy(preds, targets, task=\"multiclass\")\n        recall = torchmetrics.functional.recall(preds, targets, task=\"multiclass\")\n\n        self.log_dict(\n            {\n                \"val_loss\": loss.item(),\n                \"val_f1\": f1.item(),\n                \"val_acc\": acc.item(),\n                \"val_recall\": recall.item(),\n            },\n            prog_bar=True,\n            batch_size=targets.size(0),\n        )\n\n        return loss\n\n    def predict_step(self, examples, index):\n        raise NotImplementedError()\n\n    def configure_optimizers(self):\n        optimizer_cls: type[torch.optim.Optimizer] = self.hparams.get(\"optimizer_cls\")\n        lr_scheduler_cls: type[torch.optim.lr_scheduler.LRScheduler] = self.hparams.get(\n            \"lr_scheduler_cls\"\n        )\n\n        lr: float = self.hparams.get(\"lr\")\n        lr_scheduler_kwargs: dict[str, any] = self.hparams.get(\"lr_scheduler_kwargs\")\n\n        optimizer = optimizer_cls(self.parameters(), lr=lr)\n        lr_scheduler = lr_scheduler_cls(optimizer, **lr_scheduler_kwargs)\n\n        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n\n    def configure_callbacks(self):\n        swa = callbacks.StochasticWeightAveraging(swa_lrs=1e-2)\n        checkpoint = callbacks.ModelCheckpoint(\n            monitor=\"val_loss\", mode=\"min\", save_top_k=5\n        )\n        return [swa, checkpoint]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T06:44:28.219452Z","iopub.execute_input":"2024-11-12T06:44:28.219949Z","iopub.status.idle":"2024-11-12T06:44:28.252318Z","shell.execute_reply.started":"2024-11-12T06:44:28.219893Z","shell.execute_reply":"2024-11-12T06:44:28.251181Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import torch\nimport lightning as L\n\n\ntrainer = L.Trainer()\nmodel = AslTranslator(input_dim=(3, 256, 256), output_dim=29)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T06:44:29.665399Z","iopub.execute_input":"2024-11-12T06:44:29.665867Z","iopub.status.idle":"2024-11-12T06:45:18.228386Z","shell.execute_reply.started":"2024-11-12T06:44:29.665823Z","shell.execute_reply":"2024-11-12T06:45:18.226932Z"},"_kg_hide-output":true},"outputs":[{"name":"stderr","text":"INFO: GPU available: False, used: False\nINFO: TPU available: False, using: 0 TPU cores\nINFO: HPU available: False, using: 0 HPUs\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T06:45:18.230385Z","iopub.execute_input":"2024-11-12T06:45:18.230807Z","iopub.status.idle":"2024-11-12T06:45:18.239489Z","shell.execute_reply.started":"2024-11-12T06:45:18.230765Z","shell.execute_reply":"2024-11-12T06:45:18.238339Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"AslTranslator(\n  (conv): Sequential(\n    (0): Sequential(\n      (0): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1))\n      (1): GELU(approximate='none')\n      (2): Dropout(p=0.5, inplace=False)\n    )\n    (1): Sequential(\n      (0): Conv2d(6, 12, kernel_size=(3, 3), stride=(1, 1))\n      (1): GELU(approximate='none')\n      (2): Dropout(p=0.5, inplace=False)\n    )\n    (2): Sequential(\n      (0): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1))\n      (1): GELU(approximate='none')\n      (2): Dropout(p=0.5, inplace=False)\n    )\n  )\n  (linear): Sequential(\n    (0): Sequential(\n      (0): Linear(in_features=62500, out_features=31250, bias=True)\n      (1): GELU(approximate='none')\n      (2): Dropout(p=0.5, inplace=False)\n    )\n    (1): Sequential(\n      (0): Linear(in_features=31250, out_features=31250, bias=True)\n      (1): GELU(approximate='none')\n      (2): Dropout(p=0.5, inplace=False)\n    )\n    (2): Sequential(\n      (0): Linear(in_features=31250, out_features=31250, bias=True)\n      (1): GELU(approximate='none')\n      (2): Dropout(p=0.5, inplace=False)\n    )\n  )\n  (classifier): Linear(in_features=31250, out_features=29, bias=True)\n)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"train_dataset = TRAIN_DATASET.train_test_split(0.2, stratify_by_column=\"label\")\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset[\"train\"])\neval_dataloader = torch.utils.data.DataLoader(train_dataset[\"test\"])\n\ntrainer.fit(model, train_dataloader, eval_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T06:39:52.351168Z","iopub.execute_input":"2024-11-12T06:39:52.351656Z","iopub.status.idle":"2024-11-12T06:40:12.794821Z","shell.execute_reply.started":"2024-11-12T06:39:52.351599Z","shell.execute_reply":"2024-11-12T06:40:12.792967Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py:72: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\nINFO: The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(train_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m eval_dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(train_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:957\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[1;32m    956\u001b[0m \u001b[38;5;66;03m# strategy will configure model and move it to the device\u001b[39;00m\n\u001b[0;32m--> 957\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:158\u001b[0m, in \u001b[0;36mStrategy.setup\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_optimizers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_precision_plugin()\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:138\u001b[0m, in \u001b[0;36mStrategy.setup_optimizers\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates optimizers and schedulers.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    trainer: the Trainer, these optimizers should be connected to\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler_configs \u001b[38;5;241m=\u001b[39m \u001b[43m_init_optimizers_and_lr_schedulers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:179\u001b[0m, in \u001b[0;36m_init_optimizers_and_lr_schedulers\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls `LightningModule.configure_optimizers` and parses and validates the output.\"\"\"\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m call\n\u001b[0;32m--> 179\u001b[0m optim_conf \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfigure_optimizers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpl_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optim_conf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     rank_zero_warn(\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    184\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:167\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 167\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    170\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n","Cell \u001b[0;32mIn[6], line 161\u001b[0m, in \u001b[0;36mAslTranslator.configure_optimizers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m lr_scheduler_kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28many\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr_scheduler_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    160\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optimizer_cls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m--> 161\u001b[0m lr_scheduler \u001b[38;5;241m=\u001b[39m \u001b[43mlr_scheduler_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlr_scheduler_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m: optimizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr_scheduler\u001b[39m\u001b[38;5;124m\"\u001b[39m: lr_scheduler}\n","\u001b[0;31mTypeError\u001b[0m: CosineAnnealingLR.__init__() missing 1 required positional argument: 'T_max'"],"ename":"TypeError","evalue":"CosineAnnealingLR.__init__() missing 1 required positional argument: 'T_max'","output_type":"error"}],"execution_count":9}]}